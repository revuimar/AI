
'''

Implement an articial neuron. The neuron should take samples 
generated by a code from the task 1 as its input 
and predict their class membership at its output.
The neuron should be trainable through the formula presented during the lecture:
    ∆wj =ηεf′(s)xj =η(d−y)f′(wTxj)xj (1)
where:
• xj is the jth sample
• f′(s) is the derivative of the activation function evaluated 
for the jth sample 
• w are the weights associated with inputs
• η is the learning rate
• d is the expected (true) class label
• y is a class label predicted by the neuron

'''
import math
import numpy as np
from enum import IntEnum

class Function(IntEnum):
    step = 0
    sig = 1
    sin = 2
    tanh = 3
    sign = 4
    relu = 5
    leaky = 6
'''
class X:
    def a():
        print("a worked!")
    def b(self):
        print("b worked!")
    def c(self, s):
        getattr(self,s)()
    def d(s):
        getattr(X,s)()
'''

class Neuron:
    f = ["step_func",
        "sig_func",
        "sin_func",
        "tanh_func",
        "sign_func",
        "relu_func",
        "leaky_relu_func"]
    def __init__(self,_input,act_func, weights, learn_rate, expected):
        self._input = _input
        self.act_func = act_func
        self.weights = weights
        self.learn_rate = learn_rate
    
    def setInput(self,_input):
        self._input = _input
    
    def setWeight(self,weights):
        self.weights = weights
    
    def setLearnRate(self,learn_rate):
        self.learn_rate = learn_rate
    
    #neuron functions
    def step_func(self,s,isDerivative):
        if(isDerivative):
            return 1
        else:
            return 0 if s< 0 else 1
    def sig_func(self,s):
        base = 1/(1+math.exp(s))
        if(isDerivative):
            return base*(1-base)
        else:
            return base
    def sin_func(self,s):
        if(isDerivative):
            return math.cos(s)
        else:
            return math.sin(s)
    def tanh_func(self,s):
        if(isDerivative):
            return 1-(x**2)
        else:
            return math.tanh(s)
    def sign_func(self,s):
        result = 1
        if(isDerivative):
            return result
        else:
            result = -1 if s<0  else 1
            return result
    def relu_func(self,s):
        if(isDerivative):
            return 1 if s> 0 else 0
        else:
            return s if s> 0 else 0
    def leaky_relu_func(self,s):
        if(isDerivative):
            return 1 if s> 0 else 0.01
        else:
            return s if s> 0 else 0.01*s

    def calculateSum(self):
        return np.dot(self._input,self.weights)
    def calculateValue(self,isDerivative,val):
        out = getattr(self,self.f[self.act_func])(val,isDerivative)
        return out
    def calculateOutput(self,isDerivative):
        s = self.calculateSum()
        out = getattr(self,self.f[self.act_func])(s,isDerivative)
        return out
    def calculateCorrection(self,real):
        #  ∆wj = η(d−y)f′(wTxj)xj
        delta = real - self.calculateOutput(False) #(d-y)
        fPrime = self.calculateOutput(True) #f'(wTxj)
        correction = self.learn_rate * delta * fPrime * self._input
        return correction

    def performCorrection(self,real):
        correction = self.calculateCorrection(real)
        w = self.weights
        self.setWeight(w+correction)

#n = Neuron(1,Function.step,1,1,2)
#print(n.calculateSum())
#print(n.calculateOutput(False))
